{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b010703-6a13-4f1b-b201-3863dab69e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1165d7d5-bc3f-45be-ba07-87848893e5c3",
   "metadata": {},
   "source": [
    "Hacky code to pull out just the local acceleration data across joints / frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b33467b-66c6-471d-9c1d-b6cbd106b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_file(file_in, file_out):\n",
    "    file = open(file_in, \"r\")\n",
    "    full_data = file.readlines()\n",
    "    i = full_data.index(\"MOTION\\n\")\n",
    "    frames = int(full_data[i+1].strip().replace(\"Frames: \", \"\"))\n",
    "    frame_time = float(full_data[i+2].strip().replace(\"Frame Time: \", \"\"))\n",
    "    # print(frames, frame_time)\n",
    "\n",
    "    pos_data = [] # frame [ 83 joints [xpos, ypos, zpos, zrot, xrot, yrot]]]\n",
    "    for frame in full_data[i+3: ]:\n",
    "        pos_data.append( [float(x) for x in frame.split()])\n",
    "    \n",
    "    # take the derivative to get acceleration\n",
    "    pos_data = np.array(pos_data)\n",
    "    v_data = np.diff(pos_data, 1, 0)\n",
    "    acc_data = np.diff(pos_data, 2, 0)\n",
    "    \n",
    "    # How much does taking the derivative reduce the amount of non zero data?\n",
    "    total_size = np.size(pos_data)\n",
    "    # print(total_size , round(np.count_nonzero(pos_data)/total_size*100,4), round(np.count_nonzero(acc_data)/total_size*100, 4))\n",
    "    # ~ 70 -> 20%\n",
    "    \n",
    "    # cut out the first ~5 seconds on each side of the take to remove weird artifacts\n",
    "    acc_data = acc_data[500: -500]\n",
    "    \n",
    "    np.savetxt(file_out, acc_data, delimiter=',', fmt='%d')\n",
    "\n",
    "    file.close()\n",
    "    return len(acc_data), frame_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb23c92-42cf-4238-b801-df4cdf7bbfc4",
   "metadata": {},
   "source": [
    "Go through each pair of bvh files and convert to csv with new naming convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c593e768-4174-45a1-9b3b-9fadd88b2699",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"mocap_data\"\n",
    "\n",
    "frames = []\n",
    "frame_length = []\n",
    "\n",
    "# iterate through the session directories\n",
    "for session_dir in os.listdir(data_dir):\n",
    "    if not session_dir.startswith(\"session\"):\n",
    "        continue\n",
    "    session_path = os.path.join(data_dir, session_dir)\n",
    "    for take_dir in os.listdir(session_path):\n",
    "        if not take_dir.startswith(\"take\"):\n",
    "            continue\n",
    "        take_path = os.path.join(session_path, take_dir)\n",
    "        file_a, file_b =  [x for x in os.listdir(take_path) if \".bvh\" in x]\n",
    "        person_a = file_a.replace(take_dir + \"_hasFingers_\", \"\").replace(\"_scale_local.bvh\", \"\")\n",
    "        person_b = file_b.replace(take_dir + \"_hasFingers_\", \"\").replace(\"_scale_local.bvh\", \"\")\n",
    "        new_a = \"csv_data/\" + person_a + \"_\" + person_b + \"_\" + session_dir.replace(\"ession\", \"\") + take_dir.replace(\"ake\", \"\") + '.csv'\n",
    "        new_b = \"csv_data/\" + person_b + \"_\" + person_a + \"_\" + session_dir.replace(\"ession\", \"\") + take_dir.replace(\"ake\", \"\") + '.csv'\n",
    "        f, f_len = convert_file(os.path.join(take_path, file_a), new_a)\n",
    "        frames.append(f)\n",
    "        frame_length.append(f_len)\n",
    "        f, f_len\n",
    "        convert_file(os.path.join(take_path, file_b), new_b)\n",
    "        frames.append(f)\n",
    "        frame_length.append(f_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3328795f-42c7-4d90-a67e-9c095096ea72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1 1.0\n",
      "1 0\n"
     ]
    }
   ],
   "source": [
    "print(frames[0])\n",
    "\n",
    "# Do we need to chop up any of our data?\n",
    "print(np.min(frames), np.max(frames)/np.min(frames)) # 5226 min length of motion\n",
    "      \n",
    "# Are all frames captured at the same frame rate?\n",
    "print(frame_length[0], len([x for x in frame_length if x!= frame_length[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ae96c1f-f243-43ca-bc3f-a77e80dfcb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "l = 5226 #56 files\n",
    "\n",
    "# format data so that each column is a  // WHY IS THIS SOOOOO SLOW?\n",
    "for file in [x for x in os.listdir(\"csv_data\") if \".csv\" in x]:\n",
    "    d = np.loadtxt(os.path.join(\"csv_data\", file), delimiter=',')\n",
    "    for i in range(d.shape[0]//l): # use numpy reshape instead??\n",
    "        chunk = np.array(d[i*l: (i+1)*l]).flatten()\n",
    "        if (data is None):\n",
    "            data = [chunk]\n",
    "        else:\n",
    "            data.append(chunk)\n",
    "            \n",
    "data = np.transpose(np.array(data)) # each column is a series of accelerations from within a take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0c09a27-5455-49b9-8c4d-7a503260ca72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2602548, 526)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
